url <- "http://www.spiegel.de/schlagzeilen/"
content <- html(url, encoding = "utf8")
library(rvest)
content <- html(url, encoding = "utf8")
anchors <- html_nodes(content, xpath = "//a//span")
headlines <- html_text(anchors)
headlines
anchors <- html_nodes(content, xpath = "//a//span[1]")
headlines <- html_text(anchors)
headlines
anchors <- html_nodes(content, xpath = "//a//span[2]")
headlines2 <- html_text(anchors)
anchors <- html_nodes(content, xpath = "//a//span[1]")
headlines1 <- html_text(anchors)
anchors <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "headline-date", " " ))]')
ressort <- html_text(anchors)
ressort
length(hetadlines1)
length(headlines1)
length(headlines2)
anchors <- html_nodes(content, xpath = '//span')
ressort <- html_text(anchors)
ressort
url <- "http://transparency.org"
download.file(url, destfile = "transparency.html")
dir()
download.file(url, destfile = "data/exercises/transparency.html")
html <- html("data/exercises/transparency.html")
html
content <- html("data/exercises/transparency.html")
anchors <- html_nodes(content, xpath = "//a/@href")
links <- html_text(anchors)
links <- html_attr(anchors)
?html_nodes
?html_attr
xpathSApply(content, "//a/@href", xmlGetAttr, "href")
library(XML)
xpathSApply(content, "//a/@href", xmlGetAttr, "href")
xpathSApply(content, xmlGetAttr, "//a/@href", )
xpathSApply(content, xmlGetAttr, "//a/@href")
?xmlGetAttr
xpathSApply(content, "//a", xmlGetAttr, "href")
readHTMLLinks(content)
getHTMLLinks(content)
xpathSApply(content, "//a", xmlGetAttr, "href")
?html_attr()
anchors <- html_nodes(content, xpath="//a")
html_attr(anchors, "href")
xpathSApply(content, "//a", xmlGetAttr, "href")
getHTMLLinks(content)
xpath <- '//*[(@id = "Highlights")]//*[contains(concat( " ", @class, " " ), concat( " ", "headerRegular", " " ))]//a'
xpathSApply(content, xpath)
xpathSApply(content, xpath, xmlValue)
xpath <- '//hr[@class="headerRegular"]'
xpathSApply(content, xpath, xmlValue)
xpath <- '//h3[@class="headerRegular"]'
xpathSApply(content, xpath, xmlValue)
xpath <- '//*[(@id = "Highlights")]//*[contains(concat( " ", @class, " " ), concat( " ", "headerRegular", " " ))]//a'
xpathSApply(content, xpath, xmlValue)
xpath <- '//*[contains(concat( " ", @class, " " ), concat( " ", "sitemapGroup", " " )) and (((count(preceding-sibling::*) + 1) = 4) and parent::*)]//a'
xpathSApply(content, xpath, xmlValue)
tables <- readHTMLTable("http://en.wikipedia.org/wiki/List_of_MPs_elected_in_the_United_Kingdom_general_election,_1992", encoding = "utf-8")
names(tables)
mps <- tables[[4]]
head(mps)
head(mps)
names(mps) <- c("con", "name", "party")
mps <- mps[-1,]
mps <- mps[!is.na(as.character(mps$party)),]
nrow(mps)
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir")
library(stringr)
library(XML)
# look for Sirs
mps$name <- as.character(mps$name)
mps$sir <- str_detect(mps$name, "^Sir")
table(mps$party, mps$sir)
prop.table(table(mps$party, mps$sir), 1)
library(jsonlite)
library(RCurl)
feed_url <- "http://weather.yahooapis.com/forecastrss"
feed <- getForm(feed_url , .params = list(w = "2422673", u = "c"))
parsed_feed <- xmlParse(feed)
parsed_feed
xpath <- "//yweather:location|//yweather:wind|//yweather:condition"
conditions <- unlist(xpathSApply(parsed_feed, xpath, xmlAttrs))
location <- t(xpathSApply(parsed_feed, "//yweather:location", xmlAttrs))
forecasts <- t(xpathSApply(parsed_feed, "//yweather:forecast", xmlAttrs))
forecast <- merge(location, forecasts)
options(yahooid = "v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL")
baseurl <- "http://where.yahooapis.com/v1/places.q('%s')"
woeid_url <- sprintf(baseurl, URLencode("Hoboken, NJ, USA")) # careful: URL encoding!
parsed_woeid <- xmlParse((getForm(woeid_url, appid = getOption("yahooid"))))
woeid <- xpathSApply(parsed_woeid, "//*[local-name()='locality1']", xmlAttrs)[2,] # careful: namespaces!
# build wrapper function
getWeather <- function(place = "New York", ask = "current", temp = "c") {
if (!ask %in% c("current","forecast")) {
stop("Wrong ask parameter. Choose either 'current' or 'forecast'.")
}
if (!temp %in%  c("c", "f")) {
stop("Wrong temp parameter. Choose either 'c' for Celsius or 'f' for Fahrenheit.")
}
## get woeid
base_url <- "http://where.yahooapis.com/v1/places.q('%s')"
woeid_url <- sprintf(base_url, URLencode(place))
parsed_woeid <- xmlParse((getForm(woeid_url, appid = getOption("yahooid"))))
woeid <- xpathSApply(parsed_woeid, "//*[local-name()='locality1']", xmlAttrs)[2,]
## get weather feed
feed_url <- "http://weather.yahooapis.com/forecastrss"
parsed_feed <- xmlParse(getForm(feed_url, .params = list(w = woeid, u = temp)))
## get current conditions
if (ask == "current") {
xpath <- "//yweather:location|//yweather:condition"
conds <- data.frame(t(unlist(xpathSApply(parsed_feed, xpath, xmlAttrs))))
message(sprintf("The weather in %s, %s, %s is %s. Current temperature is %sÂ°%s.", conds$city, conds$region, conds$country, tolower(conds$text), conds$temp, toupper(temp)))
}
## get forecast
if (ask == "forecast") {
location <- data.frame(t(xpathSApply(parsed_feed, "//yweather:location", xmlAttrs)))
forecasts <- data.frame(t(xpathSApply(parsed_feed, "//yweather:forecast", xmlAttrs)))
message(sprintf("Weather forecast for %s, %s, %s:", location$city, location$region, location$country))
return(forecasts)
}
}
getWeather(place = "Paris", ask = "current", temp = "c")
getWeather(place = "Bamberg", ask = "forecast", temp = "c")
yahooid = "v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahooid, fname)
Sys.getenv(yahooid)
### -----------------------------
### workshop exercises
### simon munzert
### -----------------------------
library(stringr)
library(rvest)
library(XML)
library(jsonlite)
library(RCurl)
Sys.getenv(yahooid)
yahooid = "v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahooid, fname)
normalizePath("~/")
yahooid = "yahooid=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahooid, fname)
Sys.getenv(yahooid)
### -----------------------------
### workshop exercises
### simon munzert
### -----------------------------
library(stringr)
library(rvest)
library(XML)
library(jsonlite)
library(RCurl)
Sys.getenv(yahooid)
yahoo_id = "yahooid=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL"
fname <- paste0(normalizePath("~/"),".Renviron")
fname
writeLines(yahoo_id, fname)
yahoo_id = "yahooid='v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL'"
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahoo_id, fname)
Sys.getenv(yahooid)
### -----------------------------
### workshop exercises
### simon munzert
### -----------------------------
library(stringr)
library(rvest)
library(XML)
library(jsonlite)
library(RCurl)
Sys.getenv(yahoo_id)
Sys.getenv(yahooid)
yahooid = c("yahoo_id=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL")
yahoo_id = c("yahoo_id=v.m4rTvV34GgKVVL5PEAG1uIcHyKfmY8mCJjqSl7Gx3Jkp3s2B14xCc89rQYKOmN8nc.OFbL")
fname <- paste0(normalizePath("~/"),".Renviron")
writeLines(yahoo_id, fname)
getwd()
Sys.getenv("")
Sys.getenv("yahoo_id")
url <- "http://spiegel.de"
content <- html(url, encoding = "utf8")
library(rvest)
library(stringr)
library(ggmap)
# html() parses an HTML page; automatically starts a GET request
url <- "http://spiegel.de"
content <- html(url, encoding = "utf8")
class(content)
content
browseURL("http://selectorgadget.com/")
headlines <- html_nodes(content, xpath = '//*[contains(concat( " ", @class, " " ), concat( " ", "headline", " " ))]')
class(headlines)
headlines
html_text(headlines)
library(stringr)
str_replace(headlines, '\\\"', "'")
html_text(headlines)
headlines_string <- html_text(headlines)
headlines_string
str_replace(headlines_string, '\\\"', "'")
str_replace_all(headlines_string, '\\\"', "'")
html_table(content)
### -----------------------------
### ajps reviewers
### simon munzert
### -----------------------------
## goals ------------------------
# fetch list of AJPS reviewers from PDFs
# locate them on a map
## tasks ------------------------
# downloading PDF files
# importing them into R (as plain text)
# extract information via regex
# geocoding
## packages ---------------------
library(stringr) # string processing
library(rvest) # scraping suite
library(ggmap) # geocoding
## directory ---------------------
wd <- ("./data/ajpsReviewers")
dir.create(wd)
setwd(wd)
url <- "http://ajps.org/list-of-reviewers/"
browseURL(url)
content <- html(url)
anchors <- html_nodes(content, xpath = "//a")
hrefs <- html_attr(anchors, "href")
hrefs
pdfs <- hrefs[ str_detect(hrefs, "reviewers.*\\d{4}.*pdf") ]
pdfs <- pdfs[!is.na(pdfs)]
pdfs
pdf_names <- str_extract(pdfs, "\\d{4}.pdf")
pdf_names
# download pdfs
for(i in seq_along(pdfs)) {
download.file(pdfs[i], pdf_names[i], mode="wb")
}
## step 3: transform pdfs into txt data
# xpdf: http://www.foolabs.com/xpdf/download.html
# function working for windows ...
# should use system() instead of shell() on Mac/Linux
pdftotext <- function(fname){
path_to_pdftotext <-
"C:/xpdf/pdftotext.exe"
fname_txt <- str_replace(fname, ".pdf", ".txt")
command <- str_c(path_to_pdftotext,
fname,
fname_txt, sep=" ")
shell(command)
}
pdftotext(pdf_names[1])
pdftotext(pdf_names[2])
pdftotext(pdf_names[3])
pdftotext(pdf_names[4])
## step 4: import data
fname <- str_replace(pdf_names[1], ".pdf", ".txt")
rawdat <- readLines(fname)
head(rawdat)
rev13 <- rawdat %>%
str_c(collapse="") %>%
str_replace_all(pattern = "[!\f]", replacement = " ")  %>%
str_replace_all(pattern = "\\]", replacement = " ")  %>%
str_split(pattern = "\\)") %>%
unlist()
head(rev13)
rev13 <- rev13[-1]
names <- rev13 %>%
str_extract(pattern = "^.*?,") %>%
str_replace_all(pattern = " |,", replacement = " ") %>%
str_trim()
head(names)
institution <- rev13 %>%
str_extract(pattern = ",.*\\(")) %>%
institution <- rev13 %>%
str_extract(pattern = ",.*\\(") %>%
str_replace_all(pattern = " |\\(|^, ", replacement = " ") %>%
str_trim
institution
head(institution)
reviews <- rev13 %>%
str_extract(rev13, "\\(.*") %>%
str_extract(rev13, "\\d+") %>%
as.numeric()
reviews <- rev13 %>%
str_extract("\\(.*") %>%
str_extract("\\d+") %>%
as.numeric()
reviews
head(reviews)
rev13_dat <- data.frame(names = names, institution = institution, reviews = reviews)
head(rev13_dat)
## step 6: geocode reviewers/institutions
# geocoding takes a while -> save results
# 2500 requests allowed per day
if ( !file.exists("institutions2013_geo.RData")){
pos <- geocode(rev13_dat$institution)
geocodeQueryCheck()
save(pos, file="institutions2013_geo.RData")
} else {
load("institutions2013_geo.RData")
}
head(pos)
rev13_dat$lon <- pos$lon
rev13_dat$lat <- pos$lat
View(rev13_dat)
## step 7: plot reviewers, worldwide
mapWorld <- borders("world")
map <-
ggplot() +
mapWorld +
geom_point(aes(x=rev13_dat$lon, y=rev13_dat$lat) ,
color="#F54B1A90", size=3 ,
na.rm=T) +
theme_bw() +
coord_map(xlim=c(-180, 180), ylim=c(-60,70))
map
## step 8: plot reviewers, germany
url <-
"http://biogeo.ucdavis.edu/data/gadm2/R/DEU_adm1.RData"
fname <- basename(url)
if ( !file.exists(fname) ){
download.file(url, fname, mode="wb")
}
load(fname)
map2 <-
ggplot(data=gadm, aes(x=long, y=lat)) +
geom_polygon(data = gadm, aes(group=group)) +
geom_path(color="white", aes(group=group)) +
geom_point(data = rev13_dat,
aes(x = lon, y = lat),
colour = "#F54B1A70", size=5, na.rm=T) +
coord_map(xlim=c(5, 16), ylim=c(47,55.5)) +
theme_bw()
map2
